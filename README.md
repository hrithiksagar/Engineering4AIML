# ML-Notes
Other folders: 

    Notes from Group folder:
        - This is the notes from the group at IIIT hyderabad, they are very good and well explained.
    Books:
        - This folder contains some important books
    Research Papers
        - This folder contains the research papers which are useful for Multimodal Large Language building with Highloghts after reading

1. Flash Attention [Path: Flash Attention - Stanford [7 Oct 24].pdf]
    - This is a stanford lecture, part of ML Systems Course, video on Flash Attention by official developers of Flash Attention. 
    - Link: https://youtu.be/gMOAud7hZg4?list=PLSrTvUm384I9PV10koj_cqit9OfbJXEkq 

2. transformer decoder architecture notes [Hrithik].pdf 
    - This is my notes of transformer decoder architecture taught by CampusX youtuber
    - Playlist link: https://youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&si=wgG9P3JcySfX5YPu

3. QWEN2.5 Blog 
    - this is the blog notes of QWEN2.5 blog, which is a very good blog for understanding transformer architecture.
    -   link: https://qwen2.org/qwen2-5/ 
4. QWEN2 VL 11 Oct 2024.pdf
    - This is the blog notes of QWEN2 VL blog, which is a very good blog for understanding transformer architecture. 
    -   link: https://qwen2.org/vl/ 
5. 5. Transformers inferebce  optimization 14 Oct 2024.pdf  
    -  These are my notes from the article, For additional understanding of the Transformer optimization domain: 
    -  Link: https://astralord.github.io/posts/transformer-inference-optimization-toolset/  
6. 6. how does quantization work.pdf
    - Link: https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#%C2%A7the-era-of-bit-llms-bitnet
    - Notion link: https://www.notion.so/3-How-does-quantization-work-11ff92be362680b0bab1d56197be9018?pvs=4
    - Other few quantization articles:
        -  https://mccormickml.com/2024/09/14/qlora-and-4bit-quantization/
        - Umar Jamil Video: [Quantization explained with PyTorch - Post-Training Quantization, Quantization-Aware Training](https://youtu.be/0VdNflU08yA)
        - https://medium.com/@dillipprasad60/qlora-explained-a-deep-dive-into-parametric-efficient-fine-tuning-in-large-language-models-llms-c1a4794b1766
        - https://lightning.ai/pages/community/lora-insights/
        
    
7. 7. QLORA.txt
    - How does QLORA and Quantization works. 

8. Deployment of models:
    - <Link> 

8. Asychronous Functions - My notes
    - https://youtu.be/Qb9s3UiMSTA    
9. Mixture of Experts - Mixtral 8*22B 
10. Stanford LLM Lecture:
    - https://www.youtube.com/watch?v=9vM4p9NN0Ts&pp=ygUVU3RhbmZvcmQgbGxhbWEgY291cnNl 
    - 10. Stanford CS229 LLM Lecture Notes.jpg
11. Let's build the GPT Tokenizer
    - https://youtu.be/zduSFxRajkE 

12. LLAMA 3.1 Notes
    - File: Research Papers/LLAMA 3.1 Paper.pdf

13. GPT 1 Paper Notes
    - Research Papers/Papers Shared by a peer in CVIT/1. GPT 1 language_understanding_paper.pdf 

14. Reproducing GPT 2 from scratch - Youtube Resource From Andrej Karpathy
    - https://youtu.be/l8pRSuU81PU
    - My GitHub Repository of this course: https://github.com/hrithiksagar/Reproducing-GPT-2

15. Attention is all you need paper line to line implementation by Harvard NLP
    -  https://nlp.seas.harvard.edu/annotated-transformer/ 













    
